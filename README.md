# Solve_DirichletProblem_with_Neural_Network_MSc_Mathematics--Thesis
Let's begin with a brief overview of the FEM (Finite Element Method) method, then for having a better understanding of the innovative approach taken into consideration to solve similar problems with PINNs (Physics in Neural Networks).
The Finite Element Method (FEM) is a traditional numerical technique extensively used for approximating the solutions of PDE's.
The core principle of FEM lies in the discretization of the domain into smaller, manageable pieces, known as elements. The domain, or the region of interest, is divided into a mesh of elements, often triangles or quadrilaterals in 2D, and tetrahedra or hexahedra in 3D. The vertices of these elements are referred to as nodes. The solution of the PDE, typically a function of space and time, is approximated by a piecewise continuous function, constructed using the values of the solution at the nodes.
The approximated solution within each element is expressed as a linear combination of basis functions, also known as shape functions, and the nodal values of the solution. The shape functions are chosen such that they satisfy certain properties, like continuity across element boundaries and compact support. This leads to a system of algebraic equations, obtained by substituting the approximate solution into the weak form of the PDE, and applying a weighted residual method, usually the Galerkin method.
The resulting system of equations can be represented in matrix form as Kd = F, where K is the stiffness matrix, containing the contributions from each element, d is the vector of unknown nodal values, and F is the force vector, containing the contributions from the boundary conditions and external forces. The system of equations is typically sparse and symmetric, and can be solved using various direct or iterative solvers.
On the other side, due to the unprecedented computational capacity available nowadays, Machine learning (ML) starting to take hold. It involves feeding large amounts of data through a model, tweaking the model slightly after each data point, and then using this trained model to make predictions on new data. Neural networks are a type of model used in machine learning to find and represent patterns in the data. The ability to learn from data makes neural networks incredibly powerful and versatile, making them suitable for tasks ranging from image / speech recognition to natural language processing or for finding a solution to very complicated and time-consuming mathematical problems, such as solving PDE's.
The neural networks attempt to simulate the behavior of the human brain allowing it to “learn” from large amounts of data. While a neural network with a single layer can still make approximate predictions, additional hidden layers can help to capture more complex patterns and features in the data. A neural network is composed by layers of nodes (neurons). These are the basic units of computation. The nodes in each layer are connected to nodes in the previous and next layers via “weights” and “biases”, which are adjusted during training to minimize the difference between the predicted output and the actual target values. Its structure consists of an input layer (the first layer, where the data is fed into the network), one or more hidden layers (the layers between the input and output layers, where computation is performed), and the output layer (the final layer, which produces the output).
Each node in a layer receives input from all the nodes in the previous layer and sends its output to all nodes in the next layer. The output of each node is computed as a weighted sum of its inputs added with bias, passed through an activation function.
Physics Informed Neural Networks (PINNs) are a type of neural network that leverages the power of deep learning and the underlying physical laws that govern a system to make predictions.
Traditional neural networks make predictions based solely on the data fed to them, while PINNs also take into account the known physics of the problem. This is done by incorporating the physical laws, usually in the form of partial differential equations (PDE's), into the loss function of the neural network. Consequently, the neural network is trained not only to fit the data but also to satisfy the physical laws governing the system. This allows the neural network to make physically consistent predictions even in regions where there is little or no data.
The adaptability and efficiency of PINNs stand out remarkably, delivering predictions with an accuracy that is also influenced on the granularity of the discretization across the domain where we seek to approximate the PDE solution, as well as the network structure.
In this work we propose a novel method for solving PDE's with PINNs using the approximation abilities of feedforward neural networks, resulting in a differentiable, closed analytic solution. This approach employs a neural network for approximation, refining its parameters to minimize an error function. The training phase involves optimization techniques, which need gradient computation concerning network parameters. The proposed model consists of two parts: the first adheres to initial/boundary conditions without adjustable parameters, while the second involves a neural network trained to meet the differential equation. Given that a single-hidden-layer multilayer perceptron can approximate any function with high precision, it's a promising architecture for addressing differential equations and so the main reason to create this project!
